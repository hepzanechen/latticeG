{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0055, -0.0130,  0.0299,  ..., -0.0243, -0.0521,  0.0148],\n",
      "        [ 0.0618, -0.0256, -0.0008,  ...,  0.0032, -0.0145, -0.0126],\n",
      "        [-0.0740,  0.0329,  0.0527,  ..., -0.0341, -0.0177, -0.0137],\n",
      "        ...,\n",
      "        [ 0.1796, -0.0152, -0.1540,  ...,  0.0779,  0.0621,  0.0218],\n",
      "        [ 0.1073, -0.0250, -0.0565,  ...,  0.0519,  0.0247, -0.0028],\n",
      "        [ 0.0012, -0.0526,  0.0251,  ..., -0.0681, -0.0444, -0.0290]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a matrix\n",
    "matrix = torch.randn(1000, 1000, device='cuda')  # Use the GPU by specifying device='cuda'\n",
    "\n",
    "# Perform matrix inversion\n",
    "inverted_matrix = torch.linalg.inv(matrix)\n",
    "\n",
    "# Print the result\n",
    "print(inverted_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CentralBdG(Ny=3, Nx=4, t_y=2j, t_x=(1+0j), Delta=0j)\n"
     ]
    }
   ],
   "source": [
    "from hamiltonians import Central,CentralBdG\n",
    "# Example usage\n",
    "Ny = 3\n",
    "Nx = 4\n",
    "t_y = torch.tensor(2.0*1j, dtype=torch.complex64)\n",
    "t_x = torch.tensor(1.0, dtype=torch.complex64)\n",
    "Delta = torch.tensor(0, dtype=torch.complex64)\n",
    "# Create an instance of the modified Central Hamiltonian\n",
    "central_hamiltonian = CentralBdG(Ny=Ny, Nx=Nx, t_y=t_y, t_x=t_x,Delta=Delta)\n",
    "print(central_hamiltonian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below checked batched E for lead_decimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retarded Greens function:\n",
      "tensor([[[ 0.0001-4.3289e-02j, -0.0250-7.2198e-05j],\n",
      "         [-0.0250-7.2156e-05j,  0.0001-4.3289e-02j]],\n",
      "\n",
      "        [[ 0.0002-4.3288e-02j, -0.0250-1.4423e-04j],\n",
      "         [-0.0250-1.4436e-04j,  0.0002-4.3288e-02j]]])\n",
      "Advanced Greens function:\n",
      "tensor([[[ 0.0001+4.3289e-02j, -0.0250+7.2156e-05j],\n",
      "         [-0.0250+7.2198e-05j,  0.0001+4.3289e-02j]],\n",
      "\n",
      "        [[ 0.0002+4.3288e-02j, -0.0250+1.4436e-04j],\n",
      "         [-0.0250+1.4423e-04j,  0.0002+4.3288e-02j]]])\n",
      "Lesser Greens function:\n",
      "tensor([[[ 0.0000e+00+0.0866j,  2.6077e-08+0.0001j],\n",
      "         [-2.6077e-08+0.0001j,  0.0000e+00+0.0866j]],\n",
      "\n",
      "        [[ 0.0000e+00+0.0866j, -2.9802e-08+0.0003j],\n",
      "         [ 2.9802e-08+0.0003j,  0.0000e+00+0.0866j]]])\n",
      "Greater Greens function:\n",
      "tensor([[[0.+0.j, 0.-0.j],\n",
      "         [0.+0.j, 0.+0.j]],\n",
      "\n",
      "        [[0.+0.j, 0.+0.j],\n",
      "         [0.-0.j, 0.+0.j]]])\n",
      "Elapsed time: 0.0056 seconds\n"
     ]
    }
   ],
   "source": [
    "from utils.lead_decimation import lead_decimation\n",
    "from time import time\n",
    "# Define parameters\n",
    "t = torch.tensor([[20.0, 0.0], [0.0, 20.0]],dtype=torch.complex64,device='cpu' if torch.cuda.is_available() else 'cpu')\n",
    "epsilon0 = torch.tensor([[0.0, 20.0], [20.0, 0.0]], dtype=torch.complex64, device=t.device)\n",
    "E = torch.tensor([0.1,0.2])\n",
    "mu = torch.tensor(20)\n",
    "temperature = torch.tensor(1e-6)\n",
    "particle_type = 'e'\n",
    "\n",
    "# Time the function execution\n",
    "start_time = time()\n",
    "\n",
    "# Call lead_decimation to calculate Green's functions\n",
    "gLr, gLa, gLless, gLmore = lead_decimation(E, t, epsilon0, mu, temperature, particle_type)\n",
    "elapsed_time = time() - start_time\n",
    "# Display results\n",
    "print('Retarded Greens function:')\n",
    "print(gLr)\n",
    "\n",
    "print('Advanced Greens function:')\n",
    "print(gLa)\n",
    "\n",
    "print('Lesser Greens function:')\n",
    "print(gLless)\n",
    "\n",
    "print('Greater Greens function:')\n",
    "print(gLmore)\n",
    "\n",
    "print(f'Elapsed time: {elapsed_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Giv_central for batched E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1000+0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.1000+0.0100j,  0.0000+0.0000j,  2.0000+0.0000j,\n",
      "          0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [-2.0000+0.0000j,  0.0000+0.0000j,  0.1000+0.0100j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.1000+0.0100j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [-1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.1000+0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.1000+0.0100j,  0.0000+0.0000j,  2.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -2.0000+0.0000j,  0.0000+0.0000j,  0.1000+0.0100j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,\n",
      "          0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.1000+0.0100j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.1000-0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.1000-0.0100j,  0.0000+0.0000j,  2.0000+0.0000j,\n",
      "          0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -2.0000+0.0000j,  0.0000+0.0000j,  0.1000-0.0100j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.1000-0.0100j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.1000-0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.1000-0.0100j,  0.0000+0.0000j,  2.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -2.0000+0.0000j,  0.0000+0.0000j,  0.1000-0.0100j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,\n",
      "          0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.1000-0.0100j]])\n",
      "tensor([[ 0.2000+0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.2000+0.0100j,  0.0000+0.0000j,  2.0000+0.0000j,\n",
      "          0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [-2.0000+0.0000j,  0.0000+0.0000j,  0.2000+0.0100j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.2000+0.0100j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [-1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.2000+0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.2000+0.0100j,  0.0000+0.0000j,  2.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -2.0000+0.0000j,  0.0000+0.0000j,  0.2000+0.0100j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,\n",
      "          0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.2000+0.0100j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.2000-0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.2000-0.0100j,  0.0000+0.0000j,  2.0000+0.0000j,\n",
      "          0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -2.0000+0.0000j,  0.0000+0.0000j,  0.2000-0.0100j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.2000-0.0100j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.2000-0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.2000-0.0100j,  0.0000+0.0000j,  2.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -2.0000+0.0000j,  0.0000+0.0000j,  0.2000-0.0100j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,\n",
      "          0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.2000-0.0100j]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from hamiltonians import CentralBdG\n",
    "from greens_functions import construct_ginv_central\n",
    "# Define parameters for the central region\n",
    "E = torch.tensor([0.1,0.2])\n",
    "eta = torch.tensor(1e-2)\n",
    "Ny = 2  # Number of lattice sites in the y-direction\n",
    "Nx = 2  # Number of lattice sites in the x-direction\n",
    "t_y = torch.tensor(2.0, dtype=torch.complex64)  # Hopping parameter in y-direction\n",
    "t_x = torch.tensor(1.0, dtype=torch.complex64)  # Hopping parameter in x-direction\n",
    "Delta = torch.tensor(0, dtype=torch.complex64)  # Pairing potential for BdG\n",
    "\n",
    "# Create CentralBdG instance\n",
    "central_hamiltonian = CentralBdG(Ny=Ny, Nx=Nx, t_y=t_y, t_x=t_x, Delta=Delta)\n",
    "H_BdG = central_hamiltonian.H_full_BdG  # Get the Hamiltonian matri\n",
    "ginc=construct_ginv_central(H_BdG, E, eta)\n",
    "print(ginc[0])\n",
    "print(ginc[1])\n",
    "# This result need to be compare with Matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 80, 80])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.io as sio\n",
    "from hamiltonians.Central import CentralBdG\n",
    "from hamiltonians.Lead import Lead\n",
    "from greens_functions.construct_ginv_total import construct_ginv_total\n",
    "E = torch.tensor([0.1,-10])\n",
    "eta = torch.tensor(1e-2)\n",
    "# Define parameters for the central region\n",
    "Ny = 4  # Number of lattice sites in the y-direction\n",
    "Nx = 3  # Number of lattice sites in the x-direction\n",
    "t_y = torch.tensor(2.0, dtype=torch.complex64)  # Hopping parameter in y-direction\n",
    "t_x = torch.tensor(1.0, dtype=torch.complex64)  # Hopping parameter in x-direction\n",
    "Delta = torch.tensor(0, dtype=torch.complex64)  # Pairing potential for BdG\n",
    "\n",
    "# Create CentralBdG instance\n",
    "central_hamiltonian = CentralBdG(Ny=Ny, Nx=Nx, t_y=t_y, t_x=t_x, Delta=Delta)\n",
    "H_BdG = central_hamiltonian.H_full_BdG  # Get the Hamiltonian matri\n",
    "\n",
    "# Define parameters for the leads as torch tensors\n",
    "mu_values = torch.tensor([20.0, -20.0], dtype=torch.float32)  # Chemical potentials for each lead\n",
    "t_lead_central = torch.tensor(1.0, dtype=torch.float32)  # Coupling strength between lead and central region\n",
    "t_lead = torch.tensor(20.0, dtype=torch.float32)  # Hopping parameter within the lead\n",
    "temperature = torch.tensor(1e-6, dtype=torch.float32)  # Temperature\n",
    "\n",
    "# Create lead objects\n",
    "leads_info = [Lead(mu=mu, t_lead_central=t_lead_central, temperature=temperature, Ny=Ny, t_lead=t_lead) for mu in mu_values]\n",
    "\n",
    "# Set positions for leads, shifting them by one site for python range 0\n",
    "leads_info[0].position = torch.arange(Ny)\n",
    "leads_info[1].position = leads_info[0].position + Ny * (Nx - 1)\n",
    "\n",
    "\n",
    "# Construct the total G inverse matrix\n",
    "Ginv_total = construct_ginv_total(H_BdG, E, eta, leads_info)\n",
    "print(Ginv_total.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.io import savemat\n",
    "\n",
    "# Assuming Ginv_total is of shape (batch_size, N_total, N_total)\n",
    "Ginv_total_first_E = Ginv_total[1].detach().cpu().numpy()\n",
    "\n",
    "# Save to .mat file\n",
    "savemat('Ginv_total_python.mat', {'Ginv_total': Ginv_total_first_E})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genFuncZero': {'real': tensor([116.9482, 114.2998], grad_fn=<SelectBackward0>),\n",
       "  'imag': tensor([-1.1921e-07,  1.7884e-07], grad_fn=<SelectBackward0>)},\n",
       " 'gradientsZero': {1: {'real': tensor([[ 1.0431e-07,  2.0862e-07],\n",
       "           [-1.9372e-07,  0.0000e+00]], grad_fn=<PermuteBackward0>),\n",
       "   'imag': tensor([[-0.2522, -1.1974],\n",
       "           [ 0.2522,  1.1974]], grad_fn=<PermuteBackward0>)},\n",
       "  2: {'real': tensor([[[-0.2278,  0.1642],\n",
       "            [-0.5086,  0.1499]],\n",
       "   \n",
       "           [[ 0.1642, -0.2278],\n",
       "            [ 0.1499, -0.5086]]], grad_fn=<PermuteBackward0>),\n",
       "   'imag': tensor([[[-8.1956e-08, -4.1910e-08],\n",
       "            [-1.8626e-08, -1.9372e-07]],\n",
       "   \n",
       "           [[-5.4017e-08,  5.9605e-08],\n",
       "            [-3.8743e-07,  1.6019e-07]]], grad_fn=<PermuteBackward0>)},\n",
       "  3: {'real': tensor([[[[-4.3772e-08, -4.5635e-08],\n",
       "             [-4.4703e-08,  2.7008e-08]],\n",
       "   \n",
       "            [[ 0.0000e+00,  1.2107e-07],\n",
       "             [-3.7253e-09, -7.8231e-08]]],\n",
       "   \n",
       "   \n",
       "           [[[-3.4459e-08,  4.2841e-08],\n",
       "             [ 3.3528e-08,  1.8626e-09]],\n",
       "   \n",
       "            [[ 1.8999e-07, -1.8626e-09],\n",
       "             [-7.8231e-08,  1.6019e-07]]]], grad_fn=<PermuteBackward0>),\n",
       "   'imag': tensor([[[[ 0.1842, -0.1332],\n",
       "             [-0.1332,  0.1332]],\n",
       "   \n",
       "            [[-0.0610,  0.0040],\n",
       "             [ 0.0040, -0.0040]]],\n",
       "   \n",
       "   \n",
       "           [[[-0.1332,  0.1332],\n",
       "             [ 0.1332, -0.1842]],\n",
       "   \n",
       "            [[ 0.0040, -0.0040],\n",
       "             [-0.0040,  0.0610]]]], grad_fn=<PermuteBackward0>)},\n",
       "  4: {'real': tensor([[[[[ 0.1110, -0.0811],\n",
       "              [-0.0811,  0.0891]],\n",
       "   \n",
       "             [[-0.0811,  0.0891],\n",
       "              [ 0.0891, -0.0811]]],\n",
       "   \n",
       "   \n",
       "            [[[-0.1986,  0.0416],\n",
       "              [ 0.0416,  0.0047]],\n",
       "   \n",
       "             [[ 0.0416,  0.0047],\n",
       "              [ 0.0047,  0.0416]]]],\n",
       "   \n",
       "   \n",
       "   \n",
       "           [[[[-0.0811,  0.0891],\n",
       "              [ 0.0891, -0.0811]],\n",
       "   \n",
       "             [[ 0.0891, -0.0811],\n",
       "              [-0.0811,  0.1110]]],\n",
       "   \n",
       "   \n",
       "            [[[ 0.0416,  0.0047],\n",
       "              [ 0.0047,  0.0416]],\n",
       "   \n",
       "             [[ 0.0047,  0.0416],\n",
       "              [ 0.0416, -0.1986]]]]], grad_fn=<PermuteBackward0>),\n",
       "   'imag': tensor([[[[[-7.2177e-09,  5.3085e-08],\n",
       "              [ 5.1223e-08, -3.6322e-08]],\n",
       "   \n",
       "             [[ 4.7730e-08, -1.3970e-08],\n",
       "              [-1.3970e-08,  1.3970e-09]]],\n",
       "   \n",
       "   \n",
       "            [[[-1.2014e-07, -5.1223e-08],\n",
       "              [-5.1223e-08,  4.2841e-08]],\n",
       "   \n",
       "             [[-1.3597e-07,  3.5390e-08],\n",
       "              [ 3.5390e-08,  2.2352e-08]]]],\n",
       "   \n",
       "   \n",
       "   \n",
       "           [[[[ 2.6543e-08, -1.8626e-08],\n",
       "              [-1.9558e-08,  3.2131e-08]],\n",
       "   \n",
       "             [[-2.2352e-08,  3.1432e-08],\n",
       "              [ 3.1432e-08, -5.3202e-08]]],\n",
       "   \n",
       "   \n",
       "            [[[ 7.4506e-09,  5.2154e-08],\n",
       "              [ 5.2154e-08, -1.5553e-07]],\n",
       "   \n",
       "             [[ 6.5193e-08, -3.7719e-08],\n",
       "              [-3.7719e-08, -1.4808e-07]]]]], grad_fn=<PermuteBackward0>)}}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.io as sio\n",
    "from hamiltonians.Central import CentralBdG\n",
    "from hamiltonians.Lead import Lead\n",
    "from greens_functions.construct_ginv_total import construct_ginv_total\n",
    "\n",
    "# Define parameters for the central region\n",
    "Ny = 4  # Number of lattice sites in the y-direction\n",
    "Nx = 3  # Number of lattice sites in the x-direction\n",
    "t_y = torch.tensor(2.0, dtype=torch.complex64)  # Hopping parameter in y-direction\n",
    "t_x = torch.tensor(1.0, dtype=torch.complex64)  # Hopping parameter in x-direction\n",
    "Delta = torch.tensor(0, dtype=torch.complex64)  # Pairing potential for BdG\n",
    "\n",
    "# Create CentralBdG instance\n",
    "central_hamiltonian = CentralBdG(Ny=Ny, Nx=Nx, t_y=t_y, t_x=t_x, Delta=Delta)\n",
    "H_BdG = central_hamiltonian.H_full_BdG  # Get the Hamiltonian matri\n",
    "\n",
    "# Define parameters for the leads as torch tensors\n",
    "mu_values = torch.tensor([20.0, -20.0], dtype=torch.float32)  # Chemical potentials for each lead\n",
    "t_lead_central = torch.tensor(1.0, dtype=torch.float32)  # Coupling strength between lead and central region\n",
    "t_lead = torch.tensor(20.0, dtype=torch.float32)  # Hopping parameter within the lead\n",
    "temperature = torch.tensor(1e-6, dtype=torch.float32)  # Temperature\n",
    "\n",
    "# Create lead objects\n",
    "leads_info = [Lead(mu=mu, t_lead_central=t_lead_central, temperature=temperature, Ny=Ny, t_lead=t_lead) for mu in mu_values]\n",
    "\n",
    "# Set positions for leads, shifting them by one site for python range 0\n",
    "leads_info[0].position = torch.arange(Ny)\n",
    "leads_info[1].position = leads_info[0].position + Ny * (Nx - 1)\n",
    "\n",
    "# Define energy and small imaginary part for regularization\n",
    "E = torch.tensor([0.1,0.2], dtype=torch.float32)  # Energy value\n",
    "eta = torch.tensor(1e-2,dtype=torch.float32)  # Small imaginary part for regularization\n",
    "\n",
    "# Construct the total G inverse matrix\n",
    "Ginv_total = construct_ginv_total(H_BdG, E, eta, leads_info)\n",
    "\n",
    "# Print the resulting G inverse matrix\n",
    "# print(\"Ginv_total:\")\n",
    "# print(Ginv_total)\n",
    "# Save Ginv_total to a .mat file\n",
    "# sio.savemat('Ginv_total_python.mat', {'Ginv_total': Ginv_total.cpu().numpy()})\n",
    "\n",
    "# Print the resulting G inverse matrix\n",
    "# print(\"Ginv_total:\")\n",
    "# print(Ginv_total)\n",
    "from calculations.calculation_cf_autograd import calculation_cf_autograd\n",
    "calculation_cf_autograd(H_BdG, E, eta, leads_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learn behavior of the jacrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradTrackingTensor(lvl=1, value=\n",
      "    tensor(500., grad_fn=<AddBackward0>)\n",
      ")\n",
      "GradTrackingTensor(lvl=1, value=\n",
      "    tensor(500., grad_fn=<AddBackward0>)\n",
      ")\n",
      "Jacobian with respect to x:\n",
      " tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "Jacobian with respect to y:\n",
      " tensor([[ 0., 20., 40.],\n",
      "        [ 0., 20., 40.],\n",
      "        [ 0., 20., 40.],\n",
      "        [ 0., 20., 40.],\n",
      "        [ 0., 20., 40.]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.func import jacrev\n",
    "\n",
    "# Define the original multi-variable function\n",
    "def f(x, y):\n",
    "    intery = sum(y ** 2)\n",
    "    print(intery)\n",
    "    return x + intery\n",
    "\n",
    "# Initialize x and y\n",
    "x = torch.arange(5, dtype=torch.float32)\n",
    "y = torch.arange(3, dtype=torch.float32, requires_grad=True) * 10\n",
    "\n",
    "# Compute the Jacobian with respect to x\n",
    "jacobian_x = jacrev(lambda x: f(x, y))(x)  # Shape: (len(x), len(x))\n",
    "\n",
    "# Compute the Jacobian with respect to y\n",
    "jacobian_y = jacrev(lambda y: f(x, y))(y)  # Shape: (len(y), len(y))\n",
    "\n",
    "# # Expected results\n",
    "# expectedX = torch.eye(len(x))\n",
    "# expectedY = torch.diag(2 * y)\n",
    "\n",
    "# # Assertions to check if the computed Jacobians match the expected results\n",
    "# assert torch.allclose(jacobian_x, expectedX)\n",
    "# assert torch.allclose(jacobian_y, expectedY)\n",
    "\n",
    "print(\"Jacobian with respect to x:\\n\", jacobian_x)\n",
    "print(\"Jacobian with respect to y:\\n\", jacobian_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vmap to batchize E calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'calculations.calculation_cf_autograd_vmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcalculations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcalculation_cf_autograd_vmap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calculation_cf_autograd_vmap\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Define a batch of energy values and a small imaginary part for regularization\u001b[39;00m\n\u001b[1;32m      4\u001b[0m E_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Batch of energy values\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'calculations.calculation_cf_autograd_vmap'"
     ]
    }
   ],
   "source": [
    "from calculations.calculation_cf_autograd_vmap import calculation_cf_autograd_vmap\n",
    "# Define a batch of energy values and a small imaginary part for regularization\n",
    "\n",
    "E_batch = torch.tensor([0.1, 0.2, 0.3], dtype=torch.float32)  # Batch of energy values\n",
    "eta = torch.tensor(1e-2, dtype=torch.float32)  # Small imaginary part for regularization\n",
    "\n",
    "# Call the batchized calculation function\n",
    "results = calculation_cf_autograd_vmap(E_batch, H_BdG, eta, leads_info)\n",
    "\n",
    "# Print the results\n",
    "print(\"Results for batchized energy values:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utlize vmap,make single E calc to batched E which is avaliable only for higer torch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of Inputs: tensor([1., 2., 3., 4., 5.])\n",
      "Output (Square of Inputs): tensor([ 1.,  4.,  9., 16., 25.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.func import vmap\n",
    "\n",
    "# Define a simple function that computes the square of a tensor\n",
    "def square_fn(x):\n",
    "    return x * x\n",
    "\n",
    "# Create a tensor of inputs to which we want to apply the function\n",
    "batch_of_inputs = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], dtype=torch.float32)\n",
    "\n",
    "# Use torch.vmap to vectorize the square function\n",
    "vectorized_square_fn = vmap(square_fn)\n",
    "\n",
    "# Apply the vectorized function to the batch of inputs\n",
    "output = vectorized_square_fn(batch_of_inputs)\n",
    "\n",
    "# Print the input and output to see the result\n",
    "print(\"Batch of Inputs:\", batch_of_inputs)\n",
    "print(\"Output (Square of Inputs):\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculation_cf_autograd_vmap(E_batch: torch.Tensor, H_BdG: torch.Tensor, eta: torch.Tensor, leads_info: list) -> dict:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experient about keeping track grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally kron will keep grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of A: tensor([9., 9., 9.])\n",
      "Gradient of B: tensor([6., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define tensors for the experiment\n",
    "A = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "B = torch.tensor([4.0, 5.0], requires_grad=True)\n",
    "\n",
    "# Compute the Kronecker product\n",
    "C = torch.kron(A, B)\n",
    "\n",
    "# Compute a simple function involving the Kronecker product\n",
    "output = torch.sum(C)\n",
    "\n",
    "# Perform backward to compute the gradient\n",
    "output.backward()\n",
    "\n",
    "# Print gradients of A and B\n",
    "print(\"Gradient of A:\", A.grad)\n",
    "print(\"Gradient of B:\", B.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define lambda tensor with requires_grad=True to track the gradient\n",
    "lambda_ = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "# Calculate cos and sin using lambda_\n",
    "cos_lambda = torch.cos(lambda_)\n",
    "sin_lambda = -1j * torch.sin(lambda_)\n",
    "\n",
    "# Create the phase factor matrix\n",
    "phase_factor = torch.tensor([[cos_lambda, sin_lambda], [sin_lambda, cos_lambda]], dtype=torch.complex64)\n",
    "\n",
    "# Calculate a function involving the phase_factor\n",
    "output = torch.sum(torch.abs(phase_factor))\n",
    "\n",
    "# Perform backward to compute the gradient\n",
    "output.backward()\n",
    "\n",
    "# Print gradient of lambda_\n",
    "print(\"Gradient of lambda:\", lambda_.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below torch.tensor from other tenso lost grad in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mabs(phase_factor))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Perform backward to compute the gradient\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Print gradient of lambda_\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient of lambda:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lambda_\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define lambda tensor with requires_grad=True to track the gradient\n",
    "lambda_ = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "# Calculate cos and sin using lambda_\n",
    "cos_lambda = torch.cos(lambda_)\n",
    "sin_lambda = -1j * torch.sin(lambda_)\n",
    "\n",
    "# Create the phase factor matrix\n",
    "phase_factor = torch.tensor([[cos_lambda, sin_lambda], [sin_lambda, cos_lambda]], dtype=torch.complex64)\n",
    "\n",
    "# Calculate a function involving the phase_factor\n",
    "output = torch.sum(torch.abs(phase_factor))\n",
    "\n",
    "# Perform backward to compute the gradient\n",
    "output.backward()\n",
    "\n",
    "# Print gradient of lambda_\n",
    "print(\"Gradient of lambda:\", lambda_.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use stack or cat to keep grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase factor: tensor([[0.8776+0.0000j, 0.0000-0.4794j],\n",
      "        [0.0000-0.4794j, 0.8776+0.0000j]], grad_fn=<StackBackward0>)\n",
      "Gradient of lambda: tensor(0.7963)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define lambda tensor with requires_grad=True to track the gradient\n",
    "lambda_ = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Calculate cos and sin using lambda_\n",
    "cos_lambda = torch.cos(lambda_)\n",
    "sin_lambda = -1j * torch.sin(lambda_)\n",
    "\n",
    "# Stack values to form a phase factor while retaining gradient tracking\n",
    "phase_factor = torch.stack([torch.stack([cos_lambda, sin_lambda]), torch.stack([sin_lambda, cos_lambda])])\n",
    "print(\"Phase factor:\", phase_factor)\n",
    "# Calculate a function involving the phase_factor\n",
    "output = torch.sum(torch.abs(phase_factor))\n",
    "\n",
    "# Perform backward to compute the gradient\n",
    "output.backward()\n",
    "\n",
    "# Print gradient of lambda_\n",
    "print(\"Gradient of lambda:\", lambda_.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we choose to find out log det imag or real and extract it as a real value to backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Determinant: (-inf+nanj)\n",
      "Gradient w.r.t lambda_: nan\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Simplified example of lead with lambda_\n",
    "class Lead:\n",
    "    def __init__(self, lambda_):\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "# Simulate the tunneling matrix construction (tLC)\n",
    "def construct_tlc(lambda_, Ncentre, NLi):\n",
    "    # Simple phase factor matrix with lambda_ influencing the phase\n",
    "    cos_lambda = torch.cos(lambda_ / 2)\n",
    "    sin_lambda = -1j*torch.sin(lambda_ / 2)\n",
    "    \n",
    "    # Construct the phase factor for 2x2 matrix\n",
    "    phase_factor = torch.stack([torch.stack([cos_lambda, sin_lambda]), torch.stack([sin_lambda, cos_lambda])])\n",
    "    \n",
    "    # Simple tunneling matrix using Kronecker product (e.g., 2x2 matrix block structure)\n",
    "    tLC = torch.kron(phase_factor, torch.ones(Ncentre, NLi, dtype=torch.float32))\n",
    "    \n",
    "    return tLC\n",
    "\n",
    "# Main function to compute log determinant with autograd\n",
    "def compute_logdet_autograd(lambda_, Ncentre, NLi):\n",
    "    # Convert lambda_ to tensor with requires_grad=True to track the gradient\n",
    "    lambda_tensor = torch.tensor(lambda_, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    # Construct the tunneling matrix (tLC)\n",
    "    tLC = construct_tlc(lambda_tensor, Ncentre, NLi)\n",
    "    \n",
    "    # Compute the log determinant of the real part of tLC (to avoid complex gradient issues)\n",
    "    logdet = torch.logdet(tLC)  # Use real part to enable autograd\n",
    "    logdet.imag.backward()\n",
    "    # Compute gradients\n",
    "    # logdet.backward()  # Backpropagate to calculate gradients\n",
    "    \n",
    "    # Return the log determinant and the gradient of lambda_\n",
    "    return logdet.item(), lambda_tensor.grad.item()\n",
    "\n",
    "# Test the function with a lead's lambda_ value, and tunneling matrix size (Ncentre, NLi)\n",
    "lambda_ = 0.1  # Example lambda_ value\n",
    "Ncentre = 3  # Number of lattice sites in central region\n",
    "NLi = 3  # Number of lattice sites in lead\n",
    "\n",
    "logdet_value, grad_value = compute_logdet_autograd(lambda_, Ncentre, NLi)\n",
    "\n",
    "print(f\"Log Determinant: {logdet_value}\")\n",
    "print(f\"Gradient w.r.t lambda_: {grad_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for real scalar outputs but got torch.complex64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m a\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39mj,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcomplex64,requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m b\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpow(a,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(b)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/autograd/__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    332\u001b[0m     (inputs,)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/autograd/__init__.py:206\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_dtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    202\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, graph\u001b[38;5;241m.\u001b[39mGradientEdge):\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m out_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for real scalar outputs but got torch.complex64"
     ]
    }
   ],
   "source": [
    "a=torch.tensor(1+1j,dtype=torch.complex64,requires_grad=True)\n",
    "b=torch.pow(a,2)\n",
    "b.backward()\n",
    "print(b)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define a complex tensor\n",
    "a = torch.tensor(1 + 1j, dtype=torch.complex64, requires_grad=True)\n",
    "\n",
    "# Define a complex operation: b = a^2\n",
    "b = torch.pow(a, 2)\n",
    "\n",
    "# Split b into its real and imaginary parts\n",
    "b_real = b.real\n",
    "b_imag = b.imag\n",
    "\n",
    "# Compute gradients separately for the real and imaginary parts of b\n",
    "b_real.backward(retain_graph=True)  # Real part of b\n",
    "grad_real = a.grad.clone()  # Clone the gradient for the real part\n",
    "\n",
    "# Zero gradients for a before computing for the imaginary part\n",
    "a.grad = None\n",
    "\n",
    "b_imag.backward()  # Imaginary part of b\n",
    "grad_imag = a.grad.clone()  # Clone the gradient for the imaginary part\n",
    "\n",
    "# Print the computed gradients\n",
    "print(\"Gradient of real part of b with respect to a:\", grad_real)\n",
    "print(\"Gradient of imaginary part of b with respect to a:\", grad_imag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0000, grad_fn=<PowBackward0>)\n",
      "tensor(2.0000+2.0000j)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a complex tensor\n",
    "a = torch.tensor(1 + 1j, dtype=torch.complex64, requires_grad=True)\n",
    "\n",
    "# Define a complex operation: b = a^2\n",
    "b = torch.square(a.abs())\n",
    "\n",
    "\n",
    "b.backward(retain_graph=True)  # Real part of b\n",
    "\n",
    "\n",
    "\n",
    "# Print the computed gradients\n",
    "print(b)\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.+2.j)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor(1+2j)\n",
    "print(a)\n",
    "b=a.imag\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore batched E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "E=torch.tensor([1,0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5000, 1.0000])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import fermi_distribution\n",
    "mu=torch.tensor(0)\n",
    "temperature=torch.tensor(1e-6)\n",
    "fermi_distribution(E,mu,temperature,'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CentralBdG(Ny=2, Nx=2, t_y=2j, t_x=(1+0j), Delta=0j)\n"
     ]
    }
   ],
   "source": [
    "from hamiltonians import Central,CentralBdG\n",
    "# Example usage\n",
    "Ny = 2\n",
    "Nx = 2\n",
    "t_y = torch.tensor(2.0*1j, dtype=torch.complex64)\n",
    "t_x = torch.tensor(1.0, dtype=torch.complex64)\n",
    "Delta = torch.tensor(0, dtype=torch.complex64)\n",
    "# Create an instance of the modified Central Hamiltonian\n",
    "central_hamiltonian = CentralBdG(Ny=Ny, Nx=Nx, t_y=t_y, t_x=t_x,Delta=Delta)\n",
    "print(central_hamiltonian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "batch_size = E.shape[0]\n",
    "H_BdG=central_hamiltonian.H_full_BdG\n",
    "H_BdG_batched = H_BdG.unsqueeze(0).expand(batch_size, -1, -1) \n",
    "print(H_BdG_batched.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n"
     ]
    }
   ],
   "source": [
    "print(H_BdG_batched[2]-H_BdG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### know more about autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m2.0\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#  y  x  grad_outputs\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m grads_default \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault grad_outputs (weights of 1):\u001b[39m\u001b[38;5;124m\"\u001b[39m, grads_default)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#  grad_outputs \u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#  2 3\u001b[39;00m\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/autograd/__init__.py:469\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    460\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    468\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(outputs))\n\u001b[0;32m--> 469\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/autograd/__init__.py:198\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    196\u001b[0m     out_numel_is_1 \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_numel_is_1:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_dtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    202\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#  requires_grad=True \n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# \n",
    "y = x * torch.tensor([1.0, 2.0])\n",
    "\n",
    "#  y  x  grad_outputs\n",
    "grads_default = torch.autograd.grad(y, x)\n",
    "print(\"Default grad_outputs (weights of 1):\", grads_default)\n",
    "\n",
    "#  grad_outputs \n",
    "#  2 3\n",
    "grad_outputs = torch.tensor([2.0, 3.0])\n",
    "grads_custom = torch.autograd.grad(y, x, grad_outputs=grad_outputs)\n",
    "print(\"Custom grad_outputs (weights of [2, 3]):\", grads_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m grad_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(output)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Perform backpropagation\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Print the gradients\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mgrad)  \u001b[38;5;66;03m# This will print the gradient of each element in the vector output with respect to the scalar input\u001b[39;00m\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a scalar input with requires_grad=True to track gradients\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Define a function that takes a scalar and outputs a vector\n",
    "# For example, let's create a vector where each element is a function of the scalar input\n",
    "def scalar_to_vector(scalar):\n",
    "    # This could be any function of scalar that results in a vector\n",
    "    return torch.tensor([scalar, scalar**2, scalar**3])\n",
    "\n",
    "# Compute the vector output\n",
    "output = scalar_to_vector(x)\n",
    "\n",
    "# Since we want the gradients to be vectors, we need to perform a backward pass\n",
    "# with respect to each element of the output vector separately.\n",
    "# We'll create a tensor of ones with the same shape as the output to represent\n",
    "# the \"weight\" of each element in the output vector for the backward pass.\n",
    "grad_outputs = torch.ones_like(output)\n",
    "\n",
    "# Perform backpropagation\n",
    "output.backward(gradient=grad_outputs)\n",
    "\n",
    "# Print the gradients\n",
    "print(output.grad)  # This will print the gradient of each element in the vector output with respect to the scalar input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Scalar:\n",
      " tensor(36., grad_fn=<SumBackward0>)\n",
      "Gradient:\n",
      " tensor([12., 12., 12.], grad_fn=<AddBackward0>)\n",
      "Second Derivative:\n",
      " tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "Gradient: tensor([12., 12., 12.], grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      " Correct Second Derivative: tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "Column summed Second Derivative: tensor([6., 6., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def scalar_function(vector):\n",
    "    # Example scalar function: Dot product of the vector with its transpose\n",
    "    return torch.sum(torch.matmul(vector.unsqueeze(1), vector.unsqueeze(0)))\n",
    "\n",
    "# Let's create a sample vector\n",
    "input_vector = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Compute the output of the scalar function\n",
    "output_scalar = scalar_function(input_vector)\n",
    "print(\"Output Scalar:\\n\", output_scalar)\n",
    "# Compute the gradient of the scalar function with respect to the input vector\n",
    "gradient = torch.autograd.grad(output_scalar, input_vector, create_graph=True)[0]\n",
    "\n",
    "# Compute the second derivative of the scalar function with respect to the input vector\n",
    "n = len(input_vector)\n",
    "second_derivative = torch.zeros(n, n)\n",
    "for i in range(n):\n",
    "    second_derivative[:, i] = torch.autograd.grad(gradient[i], input_vector, retain_graph=True)[0]\n",
    "\n",
    "\n",
    "print(\"Gradient:\\n\", gradient)\n",
    "print(\"Second Derivative:\\n\", second_derivative)\n",
    "\n",
    "\n",
    "# Compute the derivative of the scalar function with respect to the input vector\n",
    "gradient = torch.autograd.grad(output_scalar, input_vector, create_graph=True)\n",
    "print(\"Gradient:\", gradient[0])\n",
    "\n",
    "# Compute the second derivative of the scalar function with respect to the input vector\n",
    "output = torch.ones((gradient[0].shape[0],gradient[0].shape[0]))\n",
    "print(output)\n",
    "output=torch.eye(3,3)\n",
    "second_derivative = torch.autograd.grad(outputs = gradient, inputs = input_vector,grad_outputs=(output), create_graph=True,is_grads_batched=True, retain_graph=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\" Correct Second Derivative:\", second_derivative[0])\n",
    "\n",
    "sum_second_derivative = torch.autograd.grad(outputs = gradient, inputs = input_vector,grad_outputs=(torch.Tensor([1,1,1])), create_graph=True) # with grad_outputs= (output) error : \n",
    "# Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3]) and output[0] has a shape of torch.Size([3]).\n",
    "\n",
    "\n",
    "print(\"Column summed Second Derivative:\", sum_second_derivative[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
