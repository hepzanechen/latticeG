{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0055, -0.0130,  0.0299,  ..., -0.0243, -0.0521,  0.0148],\n",
      "        [ 0.0618, -0.0256, -0.0008,  ...,  0.0032, -0.0145, -0.0126],\n",
      "        [-0.0740,  0.0329,  0.0527,  ..., -0.0341, -0.0177, -0.0137],\n",
      "        ...,\n",
      "        [ 0.1796, -0.0152, -0.1540,  ...,  0.0779,  0.0621,  0.0218],\n",
      "        [ 0.1073, -0.0250, -0.0565,  ...,  0.0519,  0.0247, -0.0028],\n",
      "        [ 0.0012, -0.0526,  0.0251,  ..., -0.0681, -0.0444, -0.0290]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a matrix\n",
    "matrix = torch.randn(1000, 1000, device='cuda')  # Use the GPU by specifying device='cuda'\n",
    "\n",
    "# Perform matrix inversion\n",
    "inverted_matrix = torch.linalg.inv(matrix)\n",
    "\n",
    "# Print the result\n",
    "print(inverted_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CentralBdG(Ny=3, Nx=4, t_y=2j, t_x=(1+0j), Delta=0j)\n"
     ]
    }
   ],
   "source": [
    "from hamiltonians import Central,CentralBdG\n",
    "# Example usage\n",
    "Ny = 3\n",
    "Nx = 4\n",
    "t_y = torch.tensor(2.0*1j, dtype=torch.complex64)\n",
    "t_x = torch.tensor(1.0, dtype=torch.complex64)\n",
    "Delta = torch.tensor(0, dtype=torch.complex64)\n",
    "# Create an instance of the modified Central Hamiltonian\n",
    "central_hamiltonian = CentralBdG(Ny=Ny, Nx=Nx, t_y=t_y, t_x=t_x,Delta=Delta)\n",
    "print(central_hamiltonian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below checked batched E for lead_decimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retarded Greens function:\n",
      "tensor([[[ 0.0001-4.3289e-02j, -0.0250-7.2198e-05j],\n",
      "         [-0.0250-7.2156e-05j,  0.0001-4.3289e-02j]],\n",
      "\n",
      "        [[ 0.0002-4.3288e-02j, -0.0250-1.4423e-04j],\n",
      "         [-0.0250-1.4436e-04j,  0.0002-4.3288e-02j]]])\n",
      "Advanced Greens function:\n",
      "tensor([[[ 0.0001+4.3289e-02j, -0.0250+7.2156e-05j],\n",
      "         [-0.0250+7.2198e-05j,  0.0001+4.3289e-02j]],\n",
      "\n",
      "        [[ 0.0002+4.3288e-02j, -0.0250+1.4436e-04j],\n",
      "         [-0.0250+1.4423e-04j,  0.0002+4.3288e-02j]]])\n",
      "Lesser Greens function:\n",
      "tensor([[[ 0.0000e+00+0.0866j,  2.6077e-08+0.0001j],\n",
      "         [-2.6077e-08+0.0001j,  0.0000e+00+0.0866j]],\n",
      "\n",
      "        [[ 0.0000e+00+0.0866j, -2.9802e-08+0.0003j],\n",
      "         [ 2.9802e-08+0.0003j,  0.0000e+00+0.0866j]]])\n",
      "Greater Greens function:\n",
      "tensor([[[0.+0.j, 0.-0.j],\n",
      "         [0.+0.j, 0.+0.j]],\n",
      "\n",
      "        [[0.+0.j, 0.+0.j],\n",
      "         [0.-0.j, 0.+0.j]]])\n",
      "Elapsed time: 0.0056 seconds\n"
     ]
    }
   ],
   "source": [
    "from utils.lead_decimation import lead_decimation\n",
    "from time import time\n",
    "# Define parameters\n",
    "t = torch.tensor([[20.0, 0.0], [0.0, 20.0]],dtype=torch.complex64,device='cpu' if torch.cuda.is_available() else 'cpu')\n",
    "epsilon0 = torch.tensor([[0.0, 20.0], [20.0, 0.0]], dtype=torch.complex64, device=t.device)\n",
    "E = torch.tensor([0.1,0.2])\n",
    "mu = torch.tensor(20)\n",
    "temperature = torch.tensor(1e-6)\n",
    "particle_type = 'e'\n",
    "\n",
    "# Time the function execution\n",
    "start_time = time()\n",
    "\n",
    "# Call lead_decimation to calculate Green's functions\n",
    "gLr, gLa, gLless, gLmore = lead_decimation(E, t, epsilon0, mu, temperature, particle_type)\n",
    "elapsed_time = time() - start_time\n",
    "# Display results\n",
    "print('Retarded Greens function:')\n",
    "print(gLr)\n",
    "\n",
    "print('Advanced Greens function:')\n",
    "print(gLa)\n",
    "\n",
    "print('Lesser Greens function:')\n",
    "print(gLless)\n",
    "\n",
    "print('Greater Greens function:')\n",
    "print(gLmore)\n",
    "\n",
    "print(f'Elapsed time: {elapsed_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Giv_central for batched E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1000+0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.1000+0.0100j,  0.0000+0.0000j,  2.0000+0.0000j,\n",
      "          0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [-2.0000+0.0000j,  0.0000+0.0000j,  0.1000+0.0100j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.1000+0.0100j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [-1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.1000+0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.1000+0.0100j,  0.0000+0.0000j,  2.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -2.0000+0.0000j,  0.0000+0.0000j,  0.1000+0.0100j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,\n",
      "          0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.1000+0.0100j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.1000-0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.1000-0.0100j,  0.0000+0.0000j,  2.0000+0.0000j,\n",
      "          0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -2.0000+0.0000j,  0.0000+0.0000j,  0.1000-0.0100j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.1000-0.0100j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.1000-0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.1000-0.0100j,  0.0000+0.0000j,  2.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -2.0000+0.0000j,  0.0000+0.0000j,  0.1000-0.0100j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,\n",
      "          0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.1000-0.0100j]])\n",
      "tensor([[ 0.2000+0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.2000+0.0100j,  0.0000+0.0000j,  2.0000+0.0000j,\n",
      "          0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [-2.0000+0.0000j,  0.0000+0.0000j,  0.2000+0.0100j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.2000+0.0100j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [-1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.2000+0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.2000+0.0100j,  0.0000+0.0000j,  2.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -2.0000+0.0000j,  0.0000+0.0000j,  0.2000+0.0100j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,\n",
      "          0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.2000+0.0100j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.2000-0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.2000-0.0100j,  0.0000+0.0000j,  2.0000+0.0000j,\n",
      "          0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -2.0000+0.0000j,  0.0000+0.0000j,  0.2000-0.0100j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.2000-0.0100j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.2000-0.0100j,  0.0000+0.0000j, -2.0000+0.0000j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.2000-0.0100j,  0.0000+0.0000j,  2.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j, -1.0000+0.0000j,  0.0000+0.0000j,\n",
      "         -2.0000+0.0000j,  0.0000+0.0000j,  0.2000-0.0100j,  0.0000+0.0000j],\n",
      "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,\n",
      "          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,\n",
      "          0.0000+0.0000j,  2.0000+0.0000j,  0.0000+0.0000j,  0.2000-0.0100j]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from hamiltonians import CentralBdG\n",
    "from greens_functions import construct_ginv_central\n",
    "# Define parameters for the central region\n",
    "E = torch.tensor([0.1,0.2])\n",
    "eta = torch.tensor(1e-2)\n",
    "Ny = 2  # Number of lattice sites in the y-direction\n",
    "Nx = 2  # Number of lattice sites in the x-direction\n",
    "t_y = torch.tensor(2.0, dtype=torch.complex64)  # Hopping parameter in y-direction\n",
    "t_x = torch.tensor(1.0, dtype=torch.complex64)  # Hopping parameter in x-direction\n",
    "Delta = torch.tensor(0, dtype=torch.complex64)  # Pairing potential for BdG\n",
    "\n",
    "# Create CentralBdG instance\n",
    "central_hamiltonian = CentralBdG(Ny=Ny, Nx=Nx, t_y=t_y, t_x=t_x, Delta=Delta)\n",
    "H_BdG = central_hamiltonian.H_full_BdG  # Get the Hamiltonian matri\n",
    "ginc=construct_ginv_central(H_BdG, E, eta)\n",
    "print(ginc[0])\n",
    "print(ginc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hamiltonians.lead_hamiltonian'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlead_decimation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lead_decimation\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhamiltonians\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlead_hamiltonian\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lead\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreens_functions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlead_greens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lead_greens\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msio\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hamiltonians.lead_hamiltonian'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from hamiltonians import Lead\n",
    "from greens_functions import add_ginv_leads\n",
    "import scipy.io as sio\n",
    "\n",
    "# Define the central Ginv as a single zero value tensor\n",
    "Ginv_central = torch.zeros((1, 1), dtype=torch.complex64)  # Central Ginv as a scalar tensor of zero\n",
    "\n",
    "# Compact way to define lead properties and create lead objects\n",
    "mu_values = [20,0,10]\n",
    "temperature = torch.tensor(1e-6, dtype=torch.float32)\n",
    "Ny = 2\n",
    "t_lead_central = torch.tensor(1.0, dtype=torch.float32)\n",
    "t_lead = torch.tensor(20.0, dtype=torch.float32)\n",
    "\n",
    "# Create lead objects using a list comprehension\n",
    "leads_info = [Lead(mu=torch.tensor(mu, dtype=torch.float32), t_lead_central=t_lead_central, \n",
    "                   temperature=temperature, Ny=Ny, t_lead=t_lead) for mu in mu_values]\n",
    "\n",
    "\n",
    "# Define the energy value E\n",
    "E = torch.tensor(0.1, dtype=torch.float32)\n",
    "\n",
    "# Calculate the block diagonal Green's function matrix for the central and lead regions\n",
    "Ginv_totalBlkdiag = add_ginv_leads(Ginv_central, leads_info, E)\n",
    "\n",
    "# Convert tensor to numpy for saving\n",
    "Ginv_numpy = Ginv_totalBlkdiag.cpu().numpy()  # Move to CPU if on GPU and convert to numpy\n",
    "\n",
    "# Save the matrix to a MAT file\n",
    "sio.savemat('Ginv_totalBlkdiag_python.mat', {'Ginv_totalBlkdiag': Ginv_numpy})\n",
    "\n",
    "print(\"Matrix saved to Ginv_totalBlkdiag_python.mat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "\n",
      "Matrix B:\n",
      "tensor([[ 0.,  5.,  0.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 0.,  9., 10.]])\n",
      "\n",
      "Kronecker Product of A and B (Matrix C):\n",
      "tensor([[ 0.,  5.,  0.,  0., 10.,  0.],\n",
      "        [ 6.,  7.,  8., 12., 14., 16.],\n",
      "        [ 0.,  9., 10.,  0., 18., 20.],\n",
      "        [ 0., 15.,  0.,  0., 20.,  0.],\n",
      "        [18., 21., 24., 24., 28., 32.],\n",
      "        [ 0., 27., 30.,  0., 36., 40.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define two simple matrices (2x2 and 3x3)\n",
    "A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "B = torch.tensor([[0, 5, 0], [6, 7, 8], [0, 9, 10]], dtype=torch.float32)\n",
    "\n",
    "# Calculate the Kronecker product of A and B\n",
    "C = torch.kron(A, B)\n",
    "\n",
    "# Display the input matrices and the result of the Kronecker product\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "print(\"\\nKronecker Product of A and B (Matrix C):\")\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[1.+0.j, 0.+4.j],\n",
      "        [2.+0.j, 5.+0.j],\n",
      "        [3.+0.j, 6.+0.j]])\n",
      "False\n",
      "tensor([[1.-0.j, 2.-0.j, 3.-0.j],\n",
      "        [0.-4.j, 5.-0.j, 6.-0.j]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([[1,2,3],[4j,5,6]])\n",
    "b=a.clone()\n",
    "print(b.is_contiguous())\n",
    "c=a.T\n",
    "print(c)\n",
    "print(c.is_contiguous())\n",
    "d=a.conj()\n",
    "print(d)\n",
    "print(d.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'genFuncValue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m results \u001b[38;5;241m=\u001b[39m calculation_cf_autograd(E, H_BdG, eta, leads_info)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Save results to .mat file for MATLAB comparison\u001b[39;00m\n\u001b[1;32m     52\u001b[0m sio\u001b[38;5;241m.\u001b[39msavemat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalculation_cf_autograd_results.mat\u001b[39m\u001b[38;5;124m'\u001b[39m, {\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenFuncValue\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenFuncValue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_order_grad\u001b[39m\u001b[38;5;124m'\u001b[39m: results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradientsZero\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond_order_grad\u001b[39m\u001b[38;5;124m'\u001b[39m: results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradientsZero\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthird_order_grad\u001b[39m\u001b[38;5;124m'\u001b[39m: results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradientsZero\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfourth_order_grad\u001b[39m\u001b[38;5;124m'\u001b[39m: results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradientsZero\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     58\u001b[0m })\n",
      "\u001b[0;31mKeyError\u001b[0m: 'genFuncValue'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.io as sio\n",
    "from hamiltonians.Central import CentralBdG\n",
    "from hamiltonians.Lead import Lead\n",
    "from greens_functions.construct_ginv_total import construct_ginv_total\n",
    "\n",
    "# Define parameters for the central region\n",
    "Ny = 4  # Number of lattice sites in the y-direction\n",
    "Nx = 3  # Number of lattice sites in the x-direction\n",
    "t_y = torch.tensor(2.0, dtype=torch.complex64)  # Hopping parameter in y-direction\n",
    "t_x = torch.tensor(1.0, dtype=torch.complex64)  # Hopping parameter in x-direction\n",
    "Delta = torch.tensor(0, dtype=torch.complex64)  # Pairing potential for BdG\n",
    "\n",
    "# Create CentralBdG instance\n",
    "central_hamiltonian = CentralBdG(Ny=Ny, Nx=Nx, t_y=t_y, t_x=t_x, Delta=Delta)\n",
    "H_BdG = central_hamiltonian.H_full_BdG  # Get the Hamiltonian matri\n",
    "\n",
    "# Define parameters for the leads as torch tensors\n",
    "mu_values = torch.tensor([20.0, -20.0], dtype=torch.float32)  # Chemical potentials for each lead\n",
    "t_lead_central = torch.tensor(1.0, dtype=torch.float32)  # Coupling strength between lead and central region\n",
    "t_lead = torch.tensor(20.0, dtype=torch.float32)  # Hopping parameter within the lead\n",
    "temperature = torch.tensor(1e-6, dtype=torch.float32)  # Temperature\n",
    "\n",
    "# Create lead objects\n",
    "leads_info = [Lead(mu=mu, t_lead_central=t_lead_central, temperature=temperature, Ny=Ny, t_lead=t_lead) for mu in mu_values]\n",
    "\n",
    "# Set positions for leads, shifting them by one site for python range 0\n",
    "leads_info[0].position = torch.arange(Ny)\n",
    "leads_info[1].position = leads_info[0].position + Ny * (Nx - 1)\n",
    "\n",
    "# Define energy and small imaginary part for regularization\n",
    "E = torch.tensor(0.1, dtype=torch.float32)  # Energy value\n",
    "eta = torch.tensor(1e-2,dtype=torch.float32)  # Small imaginary part for regularization\n",
    "\n",
    "# Construct the total G inverse matrix\n",
    "Ginv_total = construct_ginv_total(H_BdG, E, eta, leads_info)\n",
    "\n",
    "# Print the resulting G inverse matrix\n",
    "# print(\"Ginv_total:\")\n",
    "# print(Ginv_total)\n",
    "# Save Ginv_total to a .mat file\n",
    "# sio.savemat('Ginv_total_python.mat', {'Ginv_total': Ginv_total.cpu().numpy()})\n",
    "\n",
    "# Print the resulting G inverse matrix\n",
    "# print(\"Ginv_total:\")\n",
    "# print(Ginv_total)\n",
    "# Calculate the generating function and its derivatives using autograd\n",
    "from calculations.calculation_cf_autograd import calculation_cf_autograd\n",
    "results = calculation_cf_autograd(E, H_BdG, eta, leads_info)\n",
    "\n",
    "# Save results to .mat file for MATLAB comparison\n",
    "sio.savemat('calculation_cf_autograd_results.mat', {\n",
    "    'genFuncValue': results['genFuncValue'],\n",
    "    'first_order_grad': results['gradientsZero'][1].cpu().detach().numpy(),\n",
    "    'second_order_grad': results['gradientsZero'][2].cpu().detach().numpy(),\n",
    "    'third_order_grad': results['gradientsZero'][3].cpu().detach().numpy(),\n",
    "    'fourth_order_grad': results['gradientsZero'][4].cpu().detach().numpy()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genFuncValueReal': 116.9482421875,\n",
       " 'genFuncValueImag': 3.8743019104003906e-07,\n",
       " 'gradientsZero': {1: {'real': tensor([ 2.9802e-07, -4.4703e-08], grad_fn=<AddBackward0>),\n",
       "   'imag': tensor([-0.2522,  0.2522], grad_fn=<AddBackward0>)},\n",
       "  2: {'real': tensor([[-0.2278,  0.1642],\n",
       "           [ 0.1642, -0.2278]], grad_fn=<StackBackward0>),\n",
       "   'imag': tensor([[-1.1921e-07, -7.3574e-08],\n",
       "           [ 4.6566e-09, -7.4506e-08]], grad_fn=<StackBackward0>)},\n",
       "  3: {'real': tensor([[-1.3853e-07, -6.8918e-08],\n",
       "           [-1.3039e-08,  7.4506e-08],\n",
       "           [-1.1176e-08,  6.1467e-08],\n",
       "           [ 3.0734e-08, -6.8918e-08]], grad_fn=<StackBackward0>),\n",
       "   'imag': tensor([[ 0.1842, -0.1332],\n",
       "           [-0.1332,  0.1332],\n",
       "           [-0.1332,  0.1332],\n",
       "           [ 0.1332, -0.1842]], grad_fn=<StackBackward0>)},\n",
       "  4: {'real': tensor([[ 0.1110, -0.0811],\n",
       "           [-0.0811,  0.0891],\n",
       "           [-0.0811,  0.0891],\n",
       "           [ 0.0891, -0.0811],\n",
       "           [-0.0811,  0.0891],\n",
       "           [ 0.0891, -0.0811],\n",
       "           [ 0.0891, -0.0811],\n",
       "           [-0.0811,  0.1110]], grad_fn=<StackBackward0>),\n",
       "   'imag': tensor([[-1.1642e-10,  7.3574e-08],\n",
       "           [ 5.6811e-08, -5.1223e-08],\n",
       "           [ 5.4948e-08, -5.1223e-08],\n",
       "           [-7.4506e-08,  6.3097e-08],\n",
       "           [ 3.6089e-08, -4.8429e-08],\n",
       "           [-3.3528e-08,  7.4971e-08],\n",
       "           [-3.3528e-08,  7.3109e-08],\n",
       "           [ 5.9139e-08, -3.8883e-08]], grad_fn=<StackBackward0>)}}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.io as sio\n",
    "from hamiltonians.Central import CentralBdG\n",
    "from hamiltonians.Lead import Lead\n",
    "from greens_functions.construct_ginv_total import construct_ginv_total\n",
    "\n",
    "# Define parameters for the central region\n",
    "Ny = 4  # Number of lattice sites in the y-direction\n",
    "Nx = 3  # Number of lattice sites in the x-direction\n",
    "t_y = torch.tensor(2.0, dtype=torch.complex64)  # Hopping parameter in y-direction\n",
    "t_x = torch.tensor(1.0, dtype=torch.complex64)  # Hopping parameter in x-direction\n",
    "Delta = torch.tensor(0, dtype=torch.complex64)  # Pairing potential for BdG\n",
    "\n",
    "# Create CentralBdG instance\n",
    "central_hamiltonian = CentralBdG(Ny=Ny, Nx=Nx, t_y=t_y, t_x=t_x, Delta=Delta)\n",
    "H_BdG = central_hamiltonian.H_full_BdG  # Get the Hamiltonian matri\n",
    "\n",
    "# Define parameters for the leads as torch tensors\n",
    "mu_values = torch.tensor([20.0, -20.0], dtype=torch.float32)  # Chemical potentials for each lead\n",
    "t_lead_central = torch.tensor(1.0, dtype=torch.float32)  # Coupling strength between lead and central region\n",
    "t_lead = torch.tensor(20.0, dtype=torch.float32)  # Hopping parameter within the lead\n",
    "temperature = torch.tensor(1e-6, dtype=torch.float32)  # Temperature\n",
    "\n",
    "# Create lead objects\n",
    "leads_info = [Lead(mu=mu, t_lead_central=t_lead_central, temperature=temperature, Ny=Ny, t_lead=t_lead) for mu in mu_values]\n",
    "\n",
    "# Set positions for leads, shifting them by one site for python range 0\n",
    "leads_info[0].position = torch.arange(Ny)\n",
    "leads_info[1].position = leads_info[0].position + Ny * (Nx - 1)\n",
    "\n",
    "# Define energy and small imaginary part for regularization\n",
    "E = torch.tensor(0.1, dtype=torch.float32)  # Energy value\n",
    "eta = torch.tensor(1e-2,dtype=torch.float32)  # Small imaginary part for regularization\n",
    "\n",
    "# Construct the total G inverse matrix\n",
    "Ginv_total = construct_ginv_total(H_BdG, E, eta, leads_info)\n",
    "\n",
    "# Print the resulting G inverse matrix\n",
    "# print(\"Ginv_total:\")\n",
    "# print(Ginv_total)\n",
    "# Save Ginv_total to a .mat file\n",
    "# sio.savemat('Ginv_total_python.mat', {'Ginv_total': Ginv_total.cpu().numpy()})\n",
    "\n",
    "# Print the resulting G inverse matrix\n",
    "# print(\"Ginv_total:\")\n",
    "# print(Ginv_total)\n",
    "from calculations.calculation_cf_autograd import calculation_cf_autograd\n",
    "calculation_cf_autograd(E, H_BdG, eta, leads_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vmap to batchize E calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "vmap: It looks like you're attempting to use a Tensor in some data-dependent control flow. We don't support that yet, please shout over at https://github.com/pytorch/functorch/issues/257 .",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m eta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1e-2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Small imaginary part for regularization\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Call the batchized calculation function\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcalculation_cf_autograd_vmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH_BdG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleads_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults for batchized energy values:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/lattice/latticeG/calculations/calculation_cf_autograd_vmap.py:34\u001b[0m, in \u001b[0;36mcalculation_cf_autograd_vmap\u001b[0;34m(E_batch, H_BdG, eta, leads_info)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m calculation_cf_autograd(E, H_BdG, eta, leads_info)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Use vmap to vectorize over batch of energy values\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m vectorized_results \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_energy_calc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Organize the results for easier access and analysis\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Since vmap results are nested dicts, we extract them appropriately\u001b[39;00m\n\u001b[1;32m     38\u001b[0m final_results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenFuncValuesReal\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenFuncValueReal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m vectorized_results]),\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenFuncValuesImag\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenFuncValueImag\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m vectorized_results]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     }\n\u001b[1;32m     59\u001b[0m }\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m/home/lattice/latticeG/calculations/calculation_cf_autograd_vmap.py:31\u001b[0m, in \u001b[0;36mcalculation_cf_autograd_vmap.<locals>.single_energy_calc\u001b[0;34m(E)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingle_energy_calc\u001b[39m(E):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Use the existing autograd calculation function\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcalculation_cf_autograd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH_BdG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleads_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/lattice/latticeG/calculations/calculation_cf_autograd.py:36\u001b[0m, in \u001b[0;36mcalculation_cf_autograd\u001b[0;34m(E, H_BdG, eta, leads_info)\u001b[0m\n\u001b[1;32m     33\u001b[0m     lead\u001b[38;5;241m.\u001b[39mlambda_ \u001b[38;5;241m=\u001b[39m lambda_tensor[i]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Construct Ginv_total\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m Ginv_total \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_ginv_total\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH_BdG\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH_BdG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleads_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleads_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Compute the generating function\u001b[39;00m\n\u001b[1;32m     39\u001b[0m gen_func \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogdet(Ginv_total)\n",
      "File \u001b[0;32m/home/lattice/latticeG/greens_functions/construct_ginv_total.py:35\u001b[0m, in \u001b[0;36mconstruct_ginv_total\u001b[0;34m(H_BdG, E, eta, leads_info)\u001b[0m\n\u001b[1;32m     32\u001b[0m num_leads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(leads_info)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Add leads diagonal part\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m Ginv_total_blkdiag \u001b[38;5;241m=\u001b[39m \u001b[43madd_ginv_leads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGinv_central\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleads_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Initialize tLCBlk matrix with zeros\u001b[39;00m\n\u001b[1;32m     38\u001b[0m tLC_blk \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(Ginv_total_blkdiag)\n",
      "File \u001b[0;32m/home/lattice/latticeG/greens_functions/add_ginv_leads.py:28\u001b[0m, in \u001b[0;36madd_ginv_leads\u001b[0;34m(Ginv_central, leads_info, E)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Loop over each lead to calculate and construct their Ginv matrices\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lead \u001b[38;5;129;01min\u001b[39;00m leads_info:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Calculate lead Green's functions for electrons and holes\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     gLr_e, gLa_e, gLless_e, gLmore_e \u001b[38;5;241m=\u001b[39m \u001b[43mlead_decimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43me\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     gLr_h, gLa_h, gLless_h, gLmore_h \u001b[38;5;241m=\u001b[39m lead_decimation(E, lead\u001b[38;5;241m.\u001b[39mt, lead\u001b[38;5;241m.\u001b[39mepsilon0, lead\u001b[38;5;241m.\u001b[39mmu, lead\u001b[38;5;241m.\u001b[39mtemperature, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Assemble the lead Green's functions in particle-hole space\u001b[39;00m\n",
      "File \u001b[0;32m/home/lattice/latticeG/utils/lead_decimation.py:48\u001b[0m, in \u001b[0;36mlead_decimation\u001b[0;34m(E, t, epsilon0, mu, temperature, particle_type, desired_accuracy)\u001b[0m\n\u001b[1;32m     45\u001b[0m E_mat \u001b[38;5;241m=\u001b[39m epsilon_s \u001b[38;5;241m+\u001b[39m (H10 \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(omega \u001b[38;5;241m-\u001b[39m H00)) \u001b[38;5;241m@\u001b[39m H01\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Iterate until desired accuracy is reached\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnorm(alpha) \u001b[38;5;241m>\u001b[39m desired_accuracy:\n\u001b[1;32m     49\u001b[0m     alpha_prev \u001b[38;5;241m=\u001b[39m alpha\n\u001b[1;32m     50\u001b[0m     beta_prev \u001b[38;5;241m=\u001b[39m beta\n",
      "\u001b[0;31mRuntimeError\u001b[0m: vmap: It looks like you're attempting to use a Tensor in some data-dependent control flow. We don't support that yet, please shout over at https://github.com/pytorch/functorch/issues/257 ."
     ]
    }
   ],
   "source": [
    "from calculations.calculation_cf_autograd_vmap import calculation_cf_autograd_vmap\n",
    "# Define a batch of energy values and a small imaginary part for regularization\n",
    "\n",
    "E_batch = torch.tensor([0.1, 0.2, 0.3], dtype=torch.float32)  # Batch of energy values\n",
    "eta = torch.tensor(1e-2, dtype=torch.float32)  # Small imaginary part for regularization\n",
    "\n",
    "# Call the batchized calculation function\n",
    "results = calculation_cf_autograd_vmap(E_batch, H_BdG, eta, leads_info)\n",
    "\n",
    "# Print the results\n",
    "print(\"Results for batchized energy values:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utlize vmap,make single E calc to batched E which is avaliable only for higer torch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of Inputs: tensor([1., 2., 3., 4., 5.])\n",
      "Output (Square of Inputs): tensor([ 1.,  4.,  9., 16., 25.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.func import vmap\n",
    "\n",
    "# Define a simple function that computes the square of a tensor\n",
    "def square_fn(x):\n",
    "    return x * x\n",
    "\n",
    "# Create a tensor of inputs to which we want to apply the function\n",
    "batch_of_inputs = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], dtype=torch.float32)\n",
    "\n",
    "# Use torch.vmap to vectorize the square function\n",
    "vectorized_square_fn = vmap(square_fn)\n",
    "\n",
    "# Apply the vectorized function to the batch of inputs\n",
    "output = vectorized_square_fn(batch_of_inputs)\n",
    "\n",
    "# Print the input and output to see the result\n",
    "print(\"Batch of Inputs:\", batch_of_inputs)\n",
    "print(\"Output (Square of Inputs):\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculation_cf_autograd_vmap(E_batch: torch.Tensor, H_BdG: torch.Tensor, eta: torch.Tensor, leads_info: list) -> dict:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experient about keeping track grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally kron will keep grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of A: tensor([9., 9., 9.])\n",
      "Gradient of B: tensor([6., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define tensors for the experiment\n",
    "A = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "B = torch.tensor([4.0, 5.0], requires_grad=True)\n",
    "\n",
    "# Compute the Kronecker product\n",
    "C = torch.kron(A, B)\n",
    "\n",
    "# Compute a simple function involving the Kronecker product\n",
    "output = torch.sum(C)\n",
    "\n",
    "# Perform backward to compute the gradient\n",
    "output.backward()\n",
    "\n",
    "# Print gradients of A and B\n",
    "print(\"Gradient of A:\", A.grad)\n",
    "print(\"Gradient of B:\", B.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define lambda tensor with requires_grad=True to track the gradient\n",
    "lambda_ = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "# Calculate cos and sin using lambda_\n",
    "cos_lambda = torch.cos(lambda_)\n",
    "sin_lambda = -1j * torch.sin(lambda_)\n",
    "\n",
    "# Create the phase factor matrix\n",
    "phase_factor = torch.tensor([[cos_lambda, sin_lambda], [sin_lambda, cos_lambda]], dtype=torch.complex64)\n",
    "\n",
    "# Calculate a function involving the phase_factor\n",
    "output = torch.sum(torch.abs(phase_factor))\n",
    "\n",
    "# Perform backward to compute the gradient\n",
    "output.backward()\n",
    "\n",
    "# Print gradient of lambda_\n",
    "print(\"Gradient of lambda:\", lambda_.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below torch.tensor from other tenso lost grad in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mabs(phase_factor))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Perform backward to compute the gradient\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Print gradient of lambda_\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient of lambda:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lambda_\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define lambda tensor with requires_grad=True to track the gradient\n",
    "lambda_ = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "# Calculate cos and sin using lambda_\n",
    "cos_lambda = torch.cos(lambda_)\n",
    "sin_lambda = -1j * torch.sin(lambda_)\n",
    "\n",
    "# Create the phase factor matrix\n",
    "phase_factor = torch.tensor([[cos_lambda, sin_lambda], [sin_lambda, cos_lambda]], dtype=torch.complex64)\n",
    "\n",
    "# Calculate a function involving the phase_factor\n",
    "output = torch.sum(torch.abs(phase_factor))\n",
    "\n",
    "# Perform backward to compute the gradient\n",
    "output.backward()\n",
    "\n",
    "# Print gradient of lambda_\n",
    "print(\"Gradient of lambda:\", lambda_.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use stack or cat to keep grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase factor: tensor([[0.8776+0.0000j, 0.0000-0.4794j],\n",
      "        [0.0000-0.4794j, 0.8776+0.0000j]], grad_fn=<StackBackward0>)\n",
      "Gradient of lambda: tensor(0.7963)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define lambda tensor with requires_grad=True to track the gradient\n",
    "lambda_ = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Calculate cos and sin using lambda_\n",
    "cos_lambda = torch.cos(lambda_)\n",
    "sin_lambda = -1j * torch.sin(lambda_)\n",
    "\n",
    "# Stack values to form a phase factor while retaining gradient tracking\n",
    "phase_factor = torch.stack([torch.stack([cos_lambda, sin_lambda]), torch.stack([sin_lambda, cos_lambda])])\n",
    "print(\"Phase factor:\", phase_factor)\n",
    "# Calculate a function involving the phase_factor\n",
    "output = torch.sum(torch.abs(phase_factor))\n",
    "\n",
    "# Perform backward to compute the gradient\n",
    "output.backward()\n",
    "\n",
    "# Print gradient of lambda_\n",
    "print(\"Gradient of lambda:\", lambda_.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we choose to find out log det imag or real and extract it as a real value to backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Determinant: (-inf+nanj)\n",
      "Gradient w.r.t lambda_: nan\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Simplified example of lead with lambda_\n",
    "class Lead:\n",
    "    def __init__(self, lambda_):\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "# Simulate the tunneling matrix construction (tLC)\n",
    "def construct_tlc(lambda_, Ncentre, NLi):\n",
    "    # Simple phase factor matrix with lambda_ influencing the phase\n",
    "    cos_lambda = torch.cos(lambda_ / 2)\n",
    "    sin_lambda = -1j*torch.sin(lambda_ / 2)\n",
    "    \n",
    "    # Construct the phase factor for 2x2 matrix\n",
    "    phase_factor = torch.stack([torch.stack([cos_lambda, sin_lambda]), torch.stack([sin_lambda, cos_lambda])])\n",
    "    \n",
    "    # Simple tunneling matrix using Kronecker product (e.g., 2x2 matrix block structure)\n",
    "    tLC = torch.kron(phase_factor, torch.ones(Ncentre, NLi, dtype=torch.float32))\n",
    "    \n",
    "    return tLC\n",
    "\n",
    "# Main function to compute log determinant with autograd\n",
    "def compute_logdet_autograd(lambda_, Ncentre, NLi):\n",
    "    # Convert lambda_ to tensor with requires_grad=True to track the gradient\n",
    "    lambda_tensor = torch.tensor(lambda_, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    # Construct the tunneling matrix (tLC)\n",
    "    tLC = construct_tlc(lambda_tensor, Ncentre, NLi)\n",
    "    \n",
    "    # Compute the log determinant of the real part of tLC (to avoid complex gradient issues)\n",
    "    logdet = torch.logdet(tLC)  # Use real part to enable autograd\n",
    "    logdet.imag.backward()\n",
    "    # Compute gradients\n",
    "    # logdet.backward()  # Backpropagate to calculate gradients\n",
    "    \n",
    "    # Return the log determinant and the gradient of lambda_\n",
    "    return logdet.item(), lambda_tensor.grad.item()\n",
    "\n",
    "# Test the function with a lead's lambda_ value, and tunneling matrix size (Ncentre, NLi)\n",
    "lambda_ = 0.1  # Example lambda_ value\n",
    "Ncentre = 3  # Number of lattice sites in central region\n",
    "NLi = 3  # Number of lattice sites in lead\n",
    "\n",
    "logdet_value, grad_value = compute_logdet_autograd(lambda_, Ncentre, NLi)\n",
    "\n",
    "print(f\"Log Determinant: {logdet_value}\")\n",
    "print(f\"Gradient w.r.t lambda_: {grad_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for real scalar outputs but got torch.complex64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m a\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39mj,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcomplex64,requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m b\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpow(a,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(b)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/autograd/__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    332\u001b[0m     (inputs,)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/home/machinelearning/ml/lib/python3.12/site-packages/torch/autograd/__init__.py:206\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_dtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    202\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, graph\u001b[38;5;241m.\u001b[39mGradientEdge):\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m out_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for real scalar outputs but got torch.complex64"
     ]
    }
   ],
   "source": [
    "a=torch.tensor(1+1j,dtype=torch.complex64,requires_grad=True)\n",
    "b=torch.pow(a,2)\n",
    "b.backward()\n",
    "print(b)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define a complex tensor\n",
    "a = torch.tensor(1 + 1j, dtype=torch.complex64, requires_grad=True)\n",
    "\n",
    "# Define a complex operation: b = a^2\n",
    "b = torch.pow(a, 2)\n",
    "\n",
    "# Split b into its real and imaginary parts\n",
    "b_real = b.real\n",
    "b_imag = b.imag\n",
    "\n",
    "# Compute gradients separately for the real and imaginary parts of b\n",
    "b_real.backward(retain_graph=True)  # Real part of b\n",
    "grad_real = a.grad.clone()  # Clone the gradient for the real part\n",
    "\n",
    "# Zero gradients for a before computing for the imaginary part\n",
    "a.grad = None\n",
    "\n",
    "b_imag.backward()  # Imaginary part of b\n",
    "grad_imag = a.grad.clone()  # Clone the gradient for the imaginary part\n",
    "\n",
    "# Print the computed gradients\n",
    "print(\"Gradient of real part of b with respect to a:\", grad_real)\n",
    "print(\"Gradient of imaginary part of b with respect to a:\", grad_imag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0000, grad_fn=<PowBackward0>)\n",
      "tensor(2.0000+2.0000j)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a complex tensor\n",
    "a = torch.tensor(1 + 1j, dtype=torch.complex64, requires_grad=True)\n",
    "\n",
    "# Define a complex operation: b = a^2\n",
    "b = torch.square(a.abs())\n",
    "\n",
    "\n",
    "b.backward(retain_graph=True)  # Real part of b\n",
    "\n",
    "\n",
    "\n",
    "# Print the computed gradients\n",
    "print(b)\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.+2.j)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor(1+2j)\n",
    "print(a)\n",
    "b=a.imag\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore batched E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "E=torch.tensor([1,0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5000, 1.0000])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import fermi_distribution\n",
    "mu=torch.tensor(0)\n",
    "temperature=torch.tensor(1e-6)\n",
    "fermi_distribution(E,mu,temperature,'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CentralBdG(Ny=2, Nx=2, t_y=2j, t_x=(1+0j), Delta=0j)\n"
     ]
    }
   ],
   "source": [
    "from hamiltonians import Central,CentralBdG\n",
    "# Example usage\n",
    "Ny = 2\n",
    "Nx = 2\n",
    "t_y = torch.tensor(2.0*1j, dtype=torch.complex64)\n",
    "t_x = torch.tensor(1.0, dtype=torch.complex64)\n",
    "Delta = torch.tensor(0, dtype=torch.complex64)\n",
    "# Create an instance of the modified Central Hamiltonian\n",
    "central_hamiltonian = CentralBdG(Ny=Ny, Nx=Nx, t_y=t_y, t_x=t_x,Delta=Delta)\n",
    "print(central_hamiltonian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "batch_size = E.shape[0]\n",
    "H_BdG=central_hamiltonian.H_full_BdG\n",
    "H_BdG_batched = H_BdG.unsqueeze(0).expand(batch_size, -1, -1) \n",
    "print(H_BdG_batched.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n"
     ]
    }
   ],
   "source": [
    "print(H_BdG_batched[2]-H_BdG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
